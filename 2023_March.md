# March 15
* What is diffusion model?  
Train a NN model to fit the denoising process.   
https://zhuanlan.zhihu.com/p/586936791  
https://wrong.wang/blog/20220605-%E4%BB%80%E4%B9%88%E6%98%AFdiffusion%E6%A8%A1%E5%9E%8B/  
https://github.com/chq1155/A-Survey-on-Generative-Diffusion-Model


# March 16
* Stable Diffusion  
Different with other diffusion model, stable diffusion diffuses latent codes to speed up the process.  
Another difference is that, during denoising process, stable diffusion introduces text information -> the NN model has two input, text's code (generated by CLIP) and latent code (generated by an Encoder).  
https://zhuanlan.zhihu.com/p/600251419  
https://github.com/CompVis/stable-diffusion  

# March 17
* Attention
![image](https://user-images.githubusercontent.com/65893273/226058610-a95ac380-1899-438e-9822-95e46394dfed.png)  
Sum with weights(带权重求和)  
step 1: compute the similarity of query(Q) and key(K) to get weights  
step 2: normalize the weights  
step 3: sum value(V) with weights  
https://easyai.tech/ai-definition/attention/  

* Transformer  
Feature extractor  (beat CNN and RNN)  
https://zhuanlan.zhihu.com/p/54743941  
https://luweikxy.gitbook.io/machine-learning-notes/self-attention-and-transformer  
